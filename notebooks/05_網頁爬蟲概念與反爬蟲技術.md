## 網頁爬蟲概念與反爬蟲技術

網頁爬蟲（Web Crawler / Web Scraper）是自動化抓取網站資料的程式，能模擬使用者瀏覽網頁的行為，並將資料提取下來。從搜尋引擎到價格比對網站，再到個人資料分析，爬蟲的應用範圍極廣。

同時，由於大量爬取可能影響網站效能，甚至涉及資料隱私與版權問題，許多網站會實施 **反爬蟲技術**（Anti-Scraping）來保護資源。

---

### 1. 網頁爬蟲的基本概念
1. **目標**  
   - 從網站收集資料，例如文字、圖片、影片、API 回傳的 JSON。
2. **原理**  
   - 向目標網站發送 HTTP 請求（GET/POST 等）。
   - 取得網頁原始碼或 API 回應。
   - 分析資料結構（HTML、JSON、XML 等）。
   - 提取並儲存資料（CSV、資料庫等）。
3. **常見用途**  
   - 搜尋引擎索引（Google、Bing）。
   - 價格比較與電商監控。
   - 新聞、社群平台內容蒐集。
   - 資料科學與 AI 訓練資料蒐集。

---

### 2. 爬蟲流程示意
1. **發送請求** → 使用 `requests`（Python）或 `axios`（Node.js）模擬瀏覽器請求。
2. **解析內容** → 透過 `BeautifulSoup`、`lxml`、`cheerio` 等工具分析 HTML。
3. **資料提取** → 根據標籤、屬性、正則表達式篩選資料。
4. **資料儲存** → 寫入 CSV、JSON、SQL 或 NoSQL 資料庫。
5. **重複與排程** → 定期抓取更新資料，可用 `cron` 或雲端任務排程。

---

### 3. 常見反爬蟲技術
網站為了防止過度爬取，可能會採用以下方法：

| 反爬手段                     | 說明 | 常見對策 |
|-----------------------------|------|----------|
| **User-Agent 檢測**         | 檢查請求標頭是否為真實瀏覽器 | 偽造 UA（`headers` 設定） |
| **IP 封鎖 / 限速**          | 過於頻繁請求會封 IP          | 限速、代理 IP、分散請求 |
| **Cookies / Session 驗證**  | 需要登入或維持會話狀態       | 模擬登入流程、儲存 Cookie |
| **JavaScript 動態渲染**     | 資料透過 JS 載入             | Selenium、Playwright、Puppeteer |
| **Captcha 驗證**            | 要求輸入驗證碼              | 手動輸入或 OCR 輔助 |
| **Token / 簽名檢查**         | 每次請求需帶動態 Token       | 分析 JS 程式碼產生 Token 邏輯 |
| **行為檢測（滑鼠、滾動）**  | 驗證使用者互動               | 自動化模擬操作腳本 |

---

### 4. 合法與道德界線
爬蟲雖然技術上可行，但必須遵守：
- **網站使用條款（Terms of Service）**
- **Robots.txt 協議**
- **當地法律**（例如 GDPR、個資法）

違反規範可能導致法律責任或封鎖。

---

### 5. 小結
- 爬蟲本質是模擬瀏覽器行為去抓取資料。
- 反爬蟲是網站保護資源與流量的手段。
- 了解反爬技術能讓爬蟲更穩定，也能幫助網站開發者設計防護機制。
```

---


